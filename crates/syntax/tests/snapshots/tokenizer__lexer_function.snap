---
source: crates/syntax/tests/tokenizer.rs
expression: lex(input)
---
[
    SpannedToken {
        span: 0..2,
        token: Normal(
            KeywordFn,
        ),
    },
    SpannedToken {
        span: 2..3,
        token: Normal(
            SigilOpenBracket,
        ),
    },
    SpannedToken {
        span: 3..4,
        token: Normal(
            Identifier(
                "a",
            ),
        ),
    },
    SpannedToken {
        span: 4..5,
        token: Normal(
            SigilColon,
        ),
    },
    SpannedToken {
        span: 6..12,
        token: Normal(
            Identifier(
                "Length",
            ),
        ),
    },
    SpannedToken {
        span: 12..13,
        token: Normal(
            SigilCloseBracket,
        ),
    },
    SpannedToken {
        span: 14..16,
        token: Normal(
            SigilSingleArrow,
        ),
    },
    SpannedToken {
        span: 17..23,
        token: Normal(
            Identifier(
                "Length",
            ),
        ),
    },
    SpannedToken {
        span: 24..25,
        token: Normal(
            SigilOpenCurlyBracket,
        ),
    },
    SpannedToken {
        span: 25..26,
        token: Normal(
            Identifier(
                "a",
            ),
        ),
    },
    SpannedToken {
        span: 27..28,
        token: Normal(
            OperatorMultiply,
        ),
    },
    SpannedToken {
        span: 29..30,
        token: Normal(
            LiteralInt(
                "2",
            ),
        ),
    },
    SpannedToken {
        span: 30..31,
        token: Normal(
            SigilCloseCurlyBracket,
        ),
    },
]
